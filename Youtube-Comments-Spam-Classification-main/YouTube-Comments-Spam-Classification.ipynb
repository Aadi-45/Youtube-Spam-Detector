{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import neccessary libraries that may be useful\n",
    "\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\adity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import language processing functions\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07T06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
       "      <td>adam riyati</td>\n",
       "      <td>2013-11-07T12:37:15</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>2013-11-08T17:34:21</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z13jhp0bxqncu512g22wvzkasxmvvzjaz04</td>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>2013-11-09T08:28:43</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z13fwbwp1oujthgqj04chlngpvzmtt3r3dw</td>\n",
       "      <td>GsMega</td>\n",
       "      <td>2013-11-10T16:05:38</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    COMMENT_ID            AUTHOR  \\\n",
       "0  LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU         Julius NM   \n",
       "1  LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A       adam riyati   \n",
       "2  LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8  Evgeny Murashkin   \n",
       "3          z13jhp0bxqncu512g22wvzkasxmvvzjaz04   ElNino Melendez   \n",
       "4          z13fwbwp1oujthgqj04chlngpvzmtt3r3dw            GsMega   \n",
       "\n",
       "                  DATE                                            CONTENT  \\\n",
       "0  2013-11-07T06:20:48  Huh, anyway check out this you[tube] channel: ...   \n",
       "1  2013-11-07T12:37:15  Hey guys check out my new channel and our firs...   \n",
       "2  2013-11-08T17:34:21             just for test I have to say murdev.com   \n",
       "3  2013-11-09T08:28:43   me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
       "4  2013-11-10T16:05:38            watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
       "\n",
       "   CLASS  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data importing\n",
    "\n",
    "all_data = []\n",
    "csv_dir = './YouTube-Spam-Collection-v1/'\n",
    "csv_files = ['Youtube01-Psy.csv','Youtube02-KatyPerry.csv','Youtube03-LMFAO.csv','Youtube04-Eminem.csv','Youtube05-Shakira.csv']\n",
    "\n",
    "for file in csv_files:\n",
    "    data = pd.read_csv(csv_dir + file)\n",
    "    all_data.append(data)\n",
    "all_data = pd.concat(all_data)\n",
    "\n",
    "# Sanity checkpoint\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1005\n",
       "0     951\n",
       "Name: CLASS, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data imbalance check (no issue here)\n",
    "all_data['CLASS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             CONTENT  CLASS\n",
       "0  Huh, anyway check out this you[tube] channel: ...      1\n",
       "1  Hey guys check out my new channel and our firs...      1\n",
       "2             just for test I have to say murdev.com      1\n",
       "3   me shaking my sexy ass on my channel enjoy ^_^ ﻿      1\n",
       "4            watch?v=vtaRGgvGtWQ   Check this out .﻿      1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data preprocessing / cleaning\n",
    "\n",
    "# Only keep Comment content and Class label\n",
    "all_data.drop(['COMMENT_ID','AUTHOR','DATE'], axis=1, inplace=True, errors='ignore')\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing / cleaning\n",
    "def process_content(comment):\n",
    "    edited_comment = \" \".join(re.findall(\"[A-Za-z]+\", comment.lower()))\n",
    "    edited_comment = edited_comment.replace('\\ufeff', '')\n",
    "    edited_comment = re.sub(r\"(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)\",'http', edited_comment)\n",
    "    return edited_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    rules = [\n",
    "        {r'>\\s+': u'>'},  # remove spaces after a tag opens or closes\n",
    "        {r'\\s+': u' '},  # replace consecutive spaces\n",
    "        {r'\\s*<br\\s*/?>\\s*': u'\\n'},  # newline after a <br>\n",
    "        {r'</(div)\\s*>\\s*': u'\\n'},  # newline after </p> and </div> and <h1/>...\n",
    "        {r'</(p|h\\d)\\s*>\\s*': u'\\n\\n'},  # newline after </p> and </div> and <h1/>...\n",
    "        {r'<head>.*<\\s*(/head|body)[^>]*>': u''},  # remove <head> to </head>\n",
    "        {r'<a\\s+href=\"([^\"]+)\"[^>]*>.*</a>': r'\\1'},  # show links instead of texts\n",
    "        {r'[ \\t]*<[^<]*?/?>': u''},  # remove remaining tags\n",
    "        {r'^\\s+': u''}  # remove spaces at the beginning\n",
    "    ]\n",
    "    for rule in rules:\n",
    "        for (k, v) in rule.items():\n",
    "            regex = re.compile(k)\n",
    "            text = regex.sub(v, text)\n",
    "        text = text.rstrip()\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>PROCESSED CONTENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "      <td>huh, anyway check out this you[tube] channel: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "      <td>hey guys check out my new channel and our firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "      <td>just for test i have to say murdev.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "      <td>watch?v=vtarggvgtwq check this out .﻿</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             CONTENT  CLASS  \\\n",
       "0  Huh, anyway check out this you[tube] channel: ...      1   \n",
       "1  Hey guys check out my new channel and our firs...      1   \n",
       "2             just for test I have to say murdev.com      1   \n",
       "3   me shaking my sexy ass on my channel enjoy ^_^ ﻿      1   \n",
       "4            watch?v=vtaRGgvGtWQ   Check this out .﻿      1   \n",
       "\n",
       "                                   PROCESSED CONTENT  \n",
       "0  huh, anyway check out this you[tube] channel: ...  \n",
       "1  hey guys check out my new channel and our firs...  \n",
       "2             just for test i have to say murdev.com  \n",
       "3   me shaking my sexy ass on my channel enjoy ^_^ ﻿  \n",
       "4              watch?v=vtarggvgtwq check this out .﻿  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all_data['PROCESSED CONTENT'] = all_data['CONTENT'].apply(process_content)\n",
    "all_data['PROCESSED CONTENT'] = all_data['CONTENT'].apply(text_cleaner)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413                       me and my big sister like you﻿\n",
      "187    who else would give katy perry a good old migh...\n",
      "39     its a good song and i like her video clip, bec...\n",
      "294       :) i&#39;ll subscribe to you. you look nice :)\n",
      "428                          watch this with sound off!﻿\n",
      "                             ...                        \n",
      "89     http://www.aaas.org/tech-i/vote#view/25874/217...\n",
      "40                                     watching in 2015﻿\n",
      "269    when i hear katy singing this, i cry. the song...\n",
      "89     check out the new hot video by dante b called ...\n",
      "378    subscribe!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!...\n",
      "Name: PROCESSED CONTENT, Length: 1564, dtype: object\n",
      "413    0\n",
      "187    0\n",
      "39     0\n",
      "294    1\n",
      "428    0\n",
      "      ..\n",
      "89     1\n",
      "40     0\n",
      "269    0\n",
      "89     1\n",
      "378    1\n",
      "Name: CLASS, Length: 1564, dtype: int64\n",
      "x_train.shape = (1564,)\n",
      "x_test.shape = (392,)\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(all_data['PROCESSED CONTENT'],all_data['CLASS'], test_size=0.2, random_state=69)\n",
    "\n",
    "# Sanity checkpoint\n",
    "print(x_train)\n",
    "print(y_train)\n",
    "\n",
    "# Print the shape train and test sets\n",
    "print(\"x_train.shape = \" + str(x_train.shape))\n",
    "print(\"x_test.shape = \" + str(x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction using Counter Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "x_train_counts = count_vect.fit_transform(x_train)\n",
    "x_test_counts = count_vect.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              TF-IDF\n",
      "sister      0.628545\n",
      "big         0.515562\n",
      "me          0.295294\n",
      "like        0.286241\n",
      "you         0.240679\n",
      "my          0.239767\n",
      "and         0.233625\n",
      "pay         0.000000\n",
      "payhip      0.000000\n",
      "pe          0.000000\n",
      "paša        0.000000\n",
      "paul        0.000000\n",
      "pc          0.000000\n",
      "patriot     0.000000\n",
      "pcs         0.000000\n",
      "pdf         0.000000\n",
      "pazzi       0.000000\n",
      "peace       0.000000\n",
      "patriarchs  0.000000\n",
      "peaceful    0.000000\n",
      "peach       0.000000\n",
      "peep        0.000000\n",
      "pen         0.000000\n",
      "penis       0.000000\n",
      "penny       0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Term frequency - inverse document frequency\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tranformer = TfidfTransformer()\n",
    "x_train_tfidf = tranformer.fit_transform(x_train_counts)\n",
    "x_test_tfidf = tranformer.transform(x_test_counts)\n",
    "\n",
    "df = pd.DataFrame(x_train_tfidf[0].T.todense(), index=count_vect.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "df = df.sort_values('TF-IDF', ascending=False)\n",
    "print (df.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              TF-IDF\n",
      "sister      0.628545\n",
      "big         0.515562\n",
      "me          0.295294\n",
      "like        0.286241\n",
      "you         0.240679\n",
      "my          0.239767\n",
      "and         0.233625\n",
      "pay         0.000000\n",
      "payhip      0.000000\n",
      "pe          0.000000\n",
      "paša        0.000000\n",
      "paul        0.000000\n",
      "pc          0.000000\n",
      "patriot     0.000000\n",
      "pcs         0.000000\n",
      "pdf         0.000000\n",
      "pazzi       0.000000\n",
      "peace       0.000000\n",
      "patriarchs  0.000000\n",
      "peaceful    0.000000\n",
      "peach       0.000000\n",
      "peep        0.000000\n",
      "pen         0.000000\n",
      "penis       0.000000\n",
      "penny       0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Same as CountVectorizer + TfidfTransformer (https://towardsdatascience.com/tf-idf-explained-and-python-sklearn-implementation-b020c5e83275)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfIdfVectorizer=TfidfVectorizer(use_idf=True)\n",
    "x_train_tfidf = tfIdfVectorizer.fit_transform(x_train)\n",
    "x_test_tfidf = tfIdfVectorizer.transform(x_test)\n",
    "\n",
    "df = pd.DataFrame(x_train_tfidf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "df = df.sort_values('TF-IDF', ascending=False)\n",
    "print (df.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9744897959183674\n"
     ]
    }
   ],
   "source": [
    "# Create and train Logistic Regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_LR = LogisticRegression()\n",
    "model_LR.fit(x_train_tfidf, y_train)\n",
    "\n",
    "accuracy = model_LR.score(x_test_tfidf, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9668367346938775\n"
     ]
    }
   ],
   "source": [
    "# Create and train Random Forest Classifier model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_RFC = RandomForestClassifier()\n",
    "model_RFC.fit(x_train_tfidf,y_train)\n",
    "\n",
    "accuracy = model_RFC.score(x_test_tfidf, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9540816326530612\n"
     ]
    }
   ],
   "source": [
    "# Create and train Multi-Layer Perceptron model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model_NN = MLPClassifier(hidden_layer_sizes=(20,40,40,20), activation='relu', solver='adam', max_iter=10000)\n",
    "model_NN.fit(x_train_tfidf, y_train)\n",
    "\n",
    "accuracy = model_NN.score(x_test_tfidf, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:41:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9158163265306123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create and train XGBClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "model_XGB = XGBClassifier(objective = 'binary:logistic', max_depth = 4, alpha = 10, learning_rate = 1.0, n_estimators = 100)\n",
    "model_XGB.fit(x_train_tfidf, y_train)\n",
    "\n",
    "accuracy = model_XGB.score(x_test_tfidf, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To improve, can use Grid Search to find best parameters\n",
    "\n",
    "# # Try Grid Search with Random Forest Classifier\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# parameters = {\n",
    "#                 'n_estimators': [80, 100, 120],\n",
    "#                 'bootstrap': [True, False],\n",
    "#                 'criterion' : ['gini', 'entropy']\n",
    "#              }\n",
    "\n",
    "# model_RFC_GSCV = GridSearchCV(RandomForestClassifier(), parameters)\n",
    "# model_RFC_GSCV.fit(x_train_tfidf, y_train)\n",
    "\n",
    "# print(model_RFC_GSCV.best_params_)\n",
    "\n",
    "# accuracy = model_RFC_GSCV.score(x_test_tfidf, y_test)\n",
    "# print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, let's try Naive Bayes method.\n",
    "\n",
    "stopwords_english = stopwords.words('english') \n",
    "stemmer = PorterStemmer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tweets(result, comments, ys):\n",
    "    '''\n",
    "    Input:\n",
    "        result: a dictionary that will be used to map each pair to its frequency\n",
    "        tweets: a list of comments\n",
    "        ys: a list corresponding to the class of each comment (either 0 or 1)\n",
    "    Output:\n",
    "        result: a dictionary mapping each pair to its frequency\n",
    "    '''\n",
    "    ### START CODE HERE ###\n",
    "    for y, comment in zip(ys, comments):\n",
    "        comment_tokens = word_tokenize(process_content(comment))\n",
    "        \n",
    "        comment_stem = []\n",
    "\n",
    "        for word in comment_tokens: # Go through every word in your tokens list\n",
    "            if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):  # remove punctuation\n",
    "                \n",
    "                stem_word = stemmer.stem(word)  # stemming word\n",
    "                comment_stem.append(stem_word)  # append to the list\n",
    "                \n",
    "        for word in comment_stem:\n",
    "            # define the key, which is the word and label tuple\n",
    "            pair = (word, y)\n",
    "            \n",
    "            # if the key exists in the dictionary, increment the count\n",
    "            if pair in result:\n",
    "                result[pair] += 1\n",
    "\n",
    "            # else, if the key is new, add it to the dictionary and set the count to 1\n",
    "            else:\n",
    "                result[pair] = 1\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the freqs dictionary for later uses\n",
    "\n",
    "freqs = count_tweets({}, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary from (word, label) to how often the word appears\n",
    "        train_x: a list of tweets\n",
    "        train_y: a list of labels correponding to the comments (0,1)\n",
    "    Output:\n",
    "        logprior: the log prior. (equation 3 above)\n",
    "        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n",
    "    '''\n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # calculate V, the number of unique words in the vocabulary\n",
    "    vocab = set([pair[0] for pair in freqs.keys()])\n",
    "    V = len(vocab)\n",
    "\n",
    "    # calculate N_pos, N_neg, V_pos, V_neg\n",
    "    N_pos = N_neg = 0\n",
    "    for pair in freqs.keys():\n",
    "        # if the label is positive (greater than zero)\n",
    "        if pair[1] > 0:\n",
    "\n",
    "            # Increment the number of positive words by the count for this (word, label) pair\n",
    "            N_pos += freqs.get(pair, 1)\n",
    "\n",
    "        # else, the label is negative\n",
    "        else:\n",
    "\n",
    "            # increment the number of negative words by the count for this (word,label) pair\n",
    "            N_neg += freqs.get(pair, 1)\n",
    "    \n",
    "    # Calculate D, the number of documents\n",
    "    D = len(train_y)\n",
    "\n",
    "    # Calculate D_pos, the number of positive documents\n",
    "    D_pos = sum(train_y)\n",
    "\n",
    "    # Calculate D_neg, the number of negative documents\n",
    "    D_neg = D - D_pos\n",
    "\n",
    "    # Calculate logprior\n",
    "    logprior = np.log(D_pos) - np.log(D_neg)\n",
    "    \n",
    "    # For each word in the vocabulary...\n",
    "    for word in vocab:\n",
    "        # get the positive and negative frequency of the word\n",
    "        freq_pos = freqs.get((word, 1.0), 0)\n",
    "        freq_neg = freqs.get((word, 0.0), 0)\n",
    "\n",
    "        # calculate the probability that each word is positive, and negative\n",
    "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
    "        p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
    "\n",
    "        # calculate the log likelihood of the word\n",
    "        loglikelihood[word] = np.log(p_w_pos) - np.log(p_w_neg)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return logprior, loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprior, loglikelihood = train_naive_bayes(freqs, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict(comment, logprior, loglikelihood):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string\n",
    "        logprior: a number\n",
    "        loglikelihood: a dictionary of words mapping to numbers\n",
    "    Output:\n",
    "        p: the sum of all the logliklihoods of each word in the comment (if found in the dictionary) + logprior (a number)\n",
    "\n",
    "    '''\n",
    "    ### START CODE HERE ###\n",
    "    # process the tweet to get a list of words\n",
    "    word_l = word_tokenize(process_content(comment))\n",
    "\n",
    "    # initialize probability to zero\n",
    "    p = 0\n",
    "\n",
    "    # add the logprior\n",
    "    p += logprior\n",
    "\n",
    "    for word in word_l:\n",
    "\n",
    "        # check if the word exists in the loglikelihood dictionary\n",
    "        if word in loglikelihood:\n",
    "            # add the log likelihood of that word to the probability\n",
    "            p += loglikelihood[word]\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_bayes(x_test, y_test, logprior, loglikelihood, naive_bayes_predict=naive_bayes_predict):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        test_x: A list of tweets\n",
    "        test_y: the corresponding labels for the list of comments\n",
    "        logprior: the logprior\n",
    "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
    "    Output:\n",
    "        accuracy: (# of comments classified correctly)/(total # of tweets)\n",
    "    \"\"\"\n",
    "    accuracy = 0  # return this properly\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    y_hats = []\n",
    "    for comment in x_test:\n",
    "        # if the prediction is > 0\n",
    "        if naive_bayes_predict(comment, logprior, loglikelihood) > 0:\n",
    "            # the predicted class is 1\n",
    "            y_hat_i = 1\n",
    "        else:\n",
    "            # otherwise the predicted class is 0\n",
    "            y_hat_i = 0\n",
    "\n",
    "        # append the predicted class to the list y_hats\n",
    "        y_hats.append(y_hat_i)\n",
    "\n",
    "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
    "    error = np.sum(np.abs(y_hats - y_test)) / len(y_test)\n",
    "\n",
    "    # Accuracy is 1 minus the error\n",
    "    accuracy = 1 - error\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8775510204081632\n"
     ]
    }
   ],
   "source": [
    "print(test_naive_bayes(x_test, y_test, logprior, loglikelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Logistic Regression Model Error Analysis ===\n",
      "\n",
      "Truth Predicted Tweet\n",
      "1\t0.00\tb'follow follow vaahidmustafic like like'\n",
      "1\t0.00\tb'o peoples of the earth i have seen how you perform every form of evil at your leisure you cease not from reveling in that which i hate behold you murder the innocent day and night and plot evil against your neighbor you stand up for the rights of those who commit abomination and clap your hands as wickedness is celebrated openly in the streets o most perverse and abominable generation shall i not repay hear the word of the lord trumpetcallofgodonline co m'\n",
      "1\t0.00\tb'awesome share rteminem love the way you lie ft rihanna http ow ly zme f'\n",
      "0\t1.00\tb'if you pause at at the last millisecond you can see that that chick is about to laugh takes a few tries'\n",
      "0\t1.00\tb'if you are a person that loves real music you should listen to quot cruz supat quot he is awesome as fuck just as eminem used to be'\n",
      "1\t0.00\tb'yea stil the best wk song ever thumbs up of you think the same'\n",
      "1\t0.00\tb'aslamu lykum from pakistan'\n",
      "0\t1.00\tb'http www youtube com watch v kq zr kcpj amp t m sbest part'\n",
      "1\t0.00\tb'top three shakira songs my choice waka waka it s time for africa can t remember to forget you empire like this comment if u like shakira'\n",
      "0\t1.00\tb'i am now going to voyage to the first comment tell my family i loved them'\n",
      "\n",
      "\n",
      "=== Random Forest Classifier Error Analysis ===\n",
      "\n",
      "Truth Predicted Tweet\n",
      "1\t0.00\tb'follow follow vaahidmustafic like like'\n",
      "0\t1.00\tb'no where near one of eminems actual best songs real fans know what im talking about untitled cold wind blows welcome hell elevator business wtp almost famous to life rock bottom no apologies same song and dance without me way i am toy soldiers mosh and insane lt songs off the top of my head that s better'\n",
      "1\t0.00\tb'like this comment if you still jam out to this song after years'\n",
      "1\t0.00\tb'o peoples of the earth i have seen how you perform every form of evil at your leisure you cease not from reveling in that which i hate behold you murder the innocent day and night and plot evil against your neighbor you stand up for the rights of those who commit abomination and clap your hands as wickedness is celebrated openly in the streets o most perverse and abominable generation shall i not repay hear the word of the lord trumpetcallofgodonline co m'\n",
      "1\t0.00\tb'pleassssssssssssssss subscribeeeeeeeeee my channnnnnelll plzz'\n",
      "0\t1.00\tb'if you pause at at the last millisecond you can see that that chick is about to laugh takes a few tries'\n",
      "1\t0.00\tb'yea stil the best wk song ever thumbs up of you think the same'\n",
      "1\t0.00\tb'aslamu lykum from pakistan'\n",
      "0\t1.00\tb'http www youtube com watch v kq zr kcpj amp t m sbest part'\n",
      "0\t1.00\tb'i am now going to voyage to the first comment tell my family i loved them'\n",
      "1\t0.00\tb'share your thoughts'\n",
      "1\t0.00\tb'like and subscrib if you watch in'\n",
      "0\t1.00\tb'my honest opinion it s a very mediocre song nothing unique or special about her music lyrics or voice nothing memorable like billie jean or beat it before her millions of fans reply with hate comments i know this is a democracy and people are free to see what they want but then don t i have the right to express my opinion please don t reply with dumb comments lie if you don t like it don t watch it i just came here to see what s the buzz about million views and didn t like what i saw ok'\n",
      "\n",
      "\n",
      "=== Multi-Layer Perceptron Model Error Analysis ===\n",
      "\n",
      "Truth Predicted Tweet\n",
      "1\t0.00\tb'like this comment if you still jam out to this song after years'\n",
      "0\t1.00\tb'it s been back for quite a while now'\n",
      "0\t1.00\tb'br'\n",
      "0\t1.00\tb'if you pause at at the last millisecond you can see that that chick is about to laugh takes a few tries'\n",
      "1\t0.00\tb'check me out i m kyle i rap so yeah'\n",
      "0\t1.00\tb'if you are a person that loves real music you should listen to quot cruz supat quot he is awesome as fuck just as eminem used to be'\n",
      "1\t0.00\tb'yea stil the best wk song ever thumbs up of you think the same'\n",
      "0\t1.00\tb'why does this video have so many views because asian things are awesome and non asian countries are jelly so they try to learn from asia by looking at this video d'\n",
      "1\t0.00\tb'aslamu lykum from pakistan'\n",
      "0\t1.00\tb'http www youtube com watch v kq zr kcpj amp t m sbest part'\n",
      "1\t0.00\tb'top three shakira songs my choice waka waka it s time for africa can t remember to forget you empire like this comment if u like shakira'\n",
      "1\t0.00\tb'hey guys check this out kollektivet don t be slappin my penis i think that they deserve much more credit than they receive'\n",
      "1\t0.00\tb'i like this comment and do not kill p'\n",
      "0\t1.00\tb'i am now going to voyage to the first comment tell my family i loved them'\n",
      "0\t1.00\tb'see it all human folly right'\n",
      "1\t0.00\tb'like and subscrib if you watch in'\n",
      "0\t1.00\tb'every time i watch this mv i just so so so glad that i live in a world that don t have to worry about running from a real human eating tiger'\n",
      "0\t1.00\tb'some classsic'\n",
      "\n",
      "\n",
      "=== Naive Bayes Model Error Analysis ===\n",
      "\n",
      "Truth Predicted Tweet\n",
      "1\t0.00\tb'subscribe me plzzzzzzz plzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz'\n",
      "0\t1.00\tb'katy perry is awesome'\n",
      "1\t0.00\tb'once you have started reading do not stop if you do not subscribe to me within one day you and you re entire family will die so if you want to stay alive subscribe right now'\n",
      "0\t1.00\tb'llikee'\n",
      "0\t1.00\tb'since when has katy perry had her own youtube channel'\n",
      "1\t0.00\tb'subscribe me and i requite'\n",
      "0\t1.00\tb'fantastic'\n",
      "1\t0.00\tb'my uncle said he will stop smoking if this comment gets likes please like this comment thanks'\n",
      "1\t0.00\tb'like this comment if you still jam out to this song after years'\n",
      "1\t0.00\tb'o peoples of the earth i have seen how you perform every form of evil at your leisure you cease not from reveling in that which i hate behold you murder the innocent day and night and plot evil against your neighbor you stand up for the rights of those who commit abomination and clap your hands as wickedness is celebrated openly in the streets o most perverse and abominable generation shall i not repay hear the word of the lord trumpetcallofgodonline co m'\n",
      "0\t1.00\tb'i like the music but is anyone listening to the lyrics'\n",
      "0\t1.00\tb'it s been back for quite a while now'\n",
      "1\t0.00\tb'pleassssssssssssssss subscribeeeeeeeeee my channnnnnelll plzz'\n",
      "0\t1.00\tb'the last year of decent music'\n",
      "0\t1.00\tb'i m here to check the views holy shit'\n",
      "0\t1.00\tb'br'\n",
      "0\t1.00\tb'lip synch is terrible'\n",
      "1\t0.00\tb'subscribe please'\n",
      "0\t1.00\tb'awesome'\n",
      "0\t1.00\tb'if you are a person that loves real music you should listen to quot cruz supat quot he is awesome as fuck just as eminem used to be'\n",
      "1\t0.00\tb'yea stil the best wk song ever thumbs up of you think the same'\n",
      "1\t0.00\tb'if i get subscribers i will summon freddy mercury s ghost to whipe from the face of earth one direction and miley cirus'\n",
      "0\t1.00\tb'i liked'\n",
      "1\t0.00\tb'listen check out andrew guasch crazy sick flow i m dope that s all there is too it if you like it subscribe if not ill be with aftermath of tde soon enough one love peace'\n",
      "1\t0.00\tb'i personally have never been in a abusive relationship i probably never will i don t hit women mom has my dad used to hit my mom before he left i can relate i m writing about one at the moment subscribe to hear it every fan counts'\n",
      "0\t1.00\tb'don t mind me i m just checking what the views are up to'\n",
      "0\t1.00\tb'boooobs'\n",
      "0\t1.00\tb'awesome'\n",
      "0\t1.00\tb'why does this video have so many views because asian things are awesome and non asian countries are jelly so they try to learn from asia by looking at this video d'\n",
      "0\t1.00\tb'millioon dislikesssssssssssssssssssssssssssssssss'\n",
      "0\t1.00\tb'views'\n",
      "1\t0.00\tb'help me get subscribers by tomorrow joking don t get butt hurt'\n",
      "0\t1.00\tb'why so many disliked'\n",
      "0\t1.00\tb''\n",
      "0\t1.00\tb'gooooood'\n",
      "1\t0.00\tb'if u love rihanna subscribe me'\n",
      "0\t1.00\tb'http www youtube com watch v kq zr kcpj amp t m sbest part'\n",
      "1\t0.00\tb'top three shakira songs my choice waka waka it s time for africa can t remember to forget you empire like this comment if u like shakira'\n",
      "0\t1.00\tb'th most viewed video i guess'\n",
      "1\t0.00\tb'i like this comment and do not kill p'\n",
      "0\t1.00\tb'views near'\n",
      "0\t1.00\tb'i am now going to voyage to the first comment tell my family i loved them'\n",
      "0\t1.00\tb'see it all human folly right'\n",
      "1\t0.00\tb'could spanish people understand this any way s i how you doing subscribe to me i brake things'\n",
      "0\t1.00\tb'roaaaaarrrrrr'\n",
      "0\t1.00\tb'some classsic'\n",
      "0\t1.00\tb'new goal let s go for it'\n",
      "0\t1.00\tb'the great mother of the jungle sweet and natural i like her videos'\n"
     ]
    }
   ],
   "source": [
    "# Error analysis of the above models\n",
    "\n",
    "print('=== Logistic Regression Model Error Analysis ===\\n')\n",
    "print('Truth Predicted Tweet')\n",
    "for x, x_tfidf, y in zip(x_test, x_test_tfidf, y_test):\n",
    "    y_hat = model_LR.predict(x_tfidf)\n",
    "    if y != (np.sign(y_hat) > 0):\n",
    "        print('%d\\t%0.2f\\t%s' % (y, np.sign(y_hat) > 0, ' '.join(\n",
    "            word_tokenize(process_content(x))).encode('ascii', 'ignore')))\n",
    "        \n",
    "print('\\n\\n=== Random Forest Classifier Error Analysis ===\\n')\n",
    "print('Truth Predicted Tweet')\n",
    "for x, x_tfidf, y in zip(x_test, x_test_tfidf, y_test):\n",
    "    y_hat = model_RFC.predict(x_tfidf)\n",
    "    if y != (np.sign(y_hat) > 0):\n",
    "        print('%d\\t%0.2f\\t%s' % (y, np.sign(y_hat) > 0, ' '.join(\n",
    "            word_tokenize(process_content(x))).encode('ascii', 'ignore')))\n",
    "        \n",
    "print('\\n\\n=== Multi-Layer Perceptron Model Error Analysis ===\\n')\n",
    "print('Truth Predicted Tweet')\n",
    "for x, x_tfidf, y in zip(x_test, x_test_tfidf, y_test):\n",
    "    y_hat = model_NN.predict(x_tfidf)\n",
    "    if y != (np.sign(y_hat) > 0):\n",
    "        print('%d\\t%0.2f\\t%s' % (y, np.sign(y_hat) > 0, ' '.join(\n",
    "            word_tokenize(process_content(x))).encode('ascii', 'ignore')))\n",
    "\n",
    "print('\\n\\n=== Naive Bayes Model Error Analysis ===\\n')\n",
    "print('Truth Predicted Tweet')\n",
    "for x, y in zip(x_test, y_test):\n",
    "    y_hat = naive_bayes_predict(x, logprior, loglikelihood)\n",
    "    if y != (np.sign(y_hat) > 0):\n",
    "        print('%d\\t%0.2f\\t%s' % (y, np.sign(y_hat) > 0, ' '.join(\n",
    "            word_tokenize(process_content(x))).encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               TF-IDF\n",
      "sister       0.616851\n",
      "big          0.522706\n",
      "me           0.299386\n",
      "like         0.286187\n",
      "you          0.244014\n",
      "my           0.243089\n",
      "and          0.236862\n",
      "pared        0.000000\n",
      "palastin     0.000000\n",
      "pan          0.000000\n",
      "pander26     0.000000\n",
      "panorama     0.000000\n",
      "pant         0.000000\n",
      "paper        0.000000\n",
      "paragraph    0.000000\n",
      "paranorm     0.000000\n",
      "00           0.000000\n",
      "parodi       0.000000\n",
      "parri        0.000000\n",
      "paint        0.000000\n",
      "part         0.000000\n",
      "parti        0.000000\n",
      "partyman318  0.000000\n",
      "pass         0.000000\n",
      "passion      0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Try stemming\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "    \n",
    "stemmed_count_vect = StemmedCountVectorizer()\n",
    "\n",
    "x_train_counts = stemmed_count_vect.fit_transform(x_train)\n",
    "x_test_counts = stemmed_count_vect.transform(x_test)\n",
    "\n",
    "tranformer = TfidfTransformer()\n",
    "x_train_tfidf = tranformer.fit_transform(x_train_counts)\n",
    "x_test_tfidf = tranformer.transform(x_test_counts)\n",
    "\n",
    "df = pd.DataFrame(x_train_tfidf[0].T.todense(), index=stemmed_count_vect.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "df = df.sort_values('TF-IDF', ascending=False)\n",
    "print (df.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9744897959183674\n"
     ]
    }
   ],
   "source": [
    "# Create and train Logistic Regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_LR = LogisticRegression()\n",
    "model_LR.fit(x_train_tfidf, y_train)\n",
    "\n",
    "accuracy = model_LR.score(x_test_tfidf, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9719387755102041\n"
     ]
    }
   ],
   "source": [
    "# Create and train Random Forest Classifier model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_RFC = RandomForestClassifier()\n",
    "model_RFC.fit(x_train_tfidf,y_train)\n",
    "\n",
    "accuracy = model_RFC.score(x_test_tfidf, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9591836734693877\n"
     ]
    }
   ],
   "source": [
    "# Create and train Multi-Layer Perceptron model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model_NN = MLPClassifier(hidden_layer_sizes=(20,40,40,20), activation='relu', solver='adam', max_iter=10000)\n",
    "model_NN.fit(x_train_tfidf, y_train)\n",
    "\n",
    "accuracy = model_NN.score(x_test_tfidf, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:41:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9285714285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create and train XGBClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "model_XGB = XGBClassifier(objective = 'binary:logistic', max_depth = 4, alpha = 10, learning_rate = 1.0, n_estimators = 100)\n",
    "model_XGB.fit(x_train_tfidf, y_train)\n",
    "\n",
    "accuracy = model_XGB.score(x_test_tfidf, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\adity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\adity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Try lemmatization\n",
    "\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               TF-IDF\n",
      "sister       0.551494\n",
      "you﻿         0.494830\n",
      "big          0.467324\n",
      "me           0.269722\n",
      "like         0.259459\n",
      "my           0.217884\n",
      "and          0.212280\n",
      "!            0.000000\n",
      "pie          0.000000\n",
      "pile         0.000000\n",
      "pigment      0.000000\n",
      "piece        0.000000\n",
      "picked       0.000000\n",
      "picture      0.000000\n",
      "pimpmyviews  0.000000\n",
      "pic          0.000000\n",
      "piano        0.000000\n",
      "photograph   0.000000\n",
      "pilot        0.000000\n",
      "piss         0.000000\n",
      "pink         0.000000\n",
      "phone﻿       0.000000\n",
      "pitbull      0.000000\n",
      "pivot        0.000000\n",
      "place        0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __call__(self, text):\n",
    "        return [lemmatizer.lemmatize(t) for t in word_tokenize(text)]\n",
    "    \n",
    "lemmed_count_vect = CountVectorizer(tokenizer=LemmaTokenizer())\n",
    "\n",
    "x_train_counts = lemmed_count_vect.fit_transform(x_train)\n",
    "x_test_counts = lemmed_count_vect.transform(x_test)\n",
    "\n",
    "tranformer = TfidfTransformer()\n",
    "x_train_tfidf = tranformer.fit_transform(x_train_counts)\n",
    "x_test_tfidf = tranformer.transform(x_test_counts)\n",
    "\n",
    "df = pd.DataFrame(x_train_tfidf[0].T.todense(), index=lemmed_count_vect.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "df = df.sort_values('TF-IDF', ascending=False)\n",
    "print (df.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9540816326530612\n"
     ]
    }
   ],
   "source": [
    "# Create and train Logistic Regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_LR = LogisticRegression()\n",
    "model_LR.fit(x_train_tfidf, y_train)\n",
    "\n",
    "accuracy = model_LR.score(x_test_tfidf, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9617346938775511\n"
     ]
    }
   ],
   "source": [
    "# Create and train Random Forest Classifier model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_RFC = RandomForestClassifier()\n",
    "model_RFC.fit(x_train_tfidf,y_train)\n",
    "\n",
    "accuracy = model_RFC.score(x_test_tfidf, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9336734693877551\n"
     ]
    }
   ],
   "source": [
    "# Create and train Multi-Layer Perceptron model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model_NN = MLPClassifier(hidden_layer_sizes=(20,40,40,20), activation='relu', solver='adam', max_iter=10000)\n",
    "model_NN.fit(x_train_tfidf, y_train)\n",
    "\n",
    "accuracy = model_NN.score(x_test_tfidf, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:41:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9311224489795918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create and train XGBClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "model_XGB = XGBClassifier(objective = 'binary:logistic', max_depth = 4, alpha = 10, learning_rate = 1.0, n_estimators = 100)\n",
    "model_XGB.fit(x_train_tfidf, y_train)\n",
    "\n",
    "accuracy = model_XGB.score(x_test_tfidf, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
